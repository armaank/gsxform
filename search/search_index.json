{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"gsxform \u00b6 Wavelet scattering transforms on graphs via PyTorch gsxform is a package for constructing graph scattering transforms, leveraging PyTorch to allow for GPU based computation. Using PyTorch, gsxform offers the ability to more easily build models that use both scattering transform and neural network components. gsxform is first and foremost a research project and is being continuously refined. Behavior can potentially be unstable and consistency is not guaranteed. Installation \u00b6 Official Release \u00b6 gsxform is available on PyPi: pip install gsxform Pre-releases \u00b6 The most up-to-date version of gsxform can be installed via git: pip install git+https://github.com/armaank/gsxform.git License \u00b6 The original code of this repository is released under the BSD 3.0-Clause Licence . Modifications, adaptations and derivative work is encouraged! Citation \u00b6 If you use gsxform , please cite using the Zenodo DOI","title":"Home"},{"location":"#gsxform","text":"Wavelet scattering transforms on graphs via PyTorch gsxform is a package for constructing graph scattering transforms, leveraging PyTorch to allow for GPU based computation. Using PyTorch, gsxform offers the ability to more easily build models that use both scattering transform and neural network components. gsxform is first and foremost a research project and is being continuously refined. Behavior can potentially be unstable and consistency is not guaranteed.","title":"gsxform"},{"location":"#installation","text":"","title":"Installation"},{"location":"#official-release","text":"gsxform is available on PyPi: pip install gsxform","title":"Official Release"},{"location":"#pre-releases","text":"The most up-to-date version of gsxform can be installed via git: pip install git+https://github.com/armaank/gsxform.git","title":"Pre-releases"},{"location":"#license","text":"The original code of this repository is released under the BSD 3.0-Clause Licence . Modifications, adaptations and derivative work is encouraged!","title":"License"},{"location":"#citation","text":"If you use gsxform , please cite using the Zenodo DOI","title":"Citation"},{"location":"about/","text":"About gsxform \u00b6 Motivation \u00b6 gsxform is an effort to provide readable, flexible and computationally efficient implementations of graph scattering transform algorithms. More explicitly, this is a project started as to assist research into the effectiveness of graph scattering transforms, and models constructed with these transforms as in contest with more conventional graph neural network approaches. Licence \u00b6 The original code of this repository is released under the BSD 3.0-Clause Licence . Modifications, adaptations and derivative work is encouraged!","title":"About gsxform"},{"location":"about/#about-gsxform","text":"","title":"About gsxform"},{"location":"about/#motivation","text":"gsxform is an effort to provide readable, flexible and computationally efficient implementations of graph scattering transform algorithms. More explicitly, this is a project started as to assist research into the effectiveness of graph scattering transforms, and models constructed with these transforms as in contest with more conventional graph neural network approaches.","title":"Motivation"},{"location":"about/#licence","text":"The original code of this repository is released under the BSD 3.0-Clause Licence . Modifications, adaptations and derivative work is encouraged!","title":"Licence"},{"location":"graph/","text":"Graph utility API documentation \u00b6 adjacency_to_laplacian \u00b6 adjacency_to_laplacian ( W ) Convert an adjacency matrix into the graph Laplacian. Parameters: Name Type Description Default W torch . Tensor Batch of normalized graph adjacency matricies. required Returns: Type Description torch . Tensor Batch of graph Laplacians. Source code in gsxform/graph.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def adjacency_to_laplacian ( W : torch . Tensor ) -> torch . Tensor : \"\"\"Convert an adjacency matrix into the graph Laplacian. Parameters ---------- W: torch.Tensor Batch of normalized graph adjacency matricies. Returns ------- torch.Tensor Batch of graph Laplacians. \"\"\" L = torch . diag_embed ( W . sum ( 1 )) - W return L normalize_adjacency \u00b6 normalize_adjacency ( W ) Normalize an adjacency matrix. Parameters: Name Type Description Default W torch . Tensor Batch of adjacency matricies. required Returns: Type Description torch . Tensor Batch of normalized adjacency matricies. Source code in gsxform/graph.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def normalize_adjacency ( W : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize an adjacency matrix. Parameters ---------- W: torch.Tensor Batch of adjacency matricies. Returns ------- torch.Tensor Batch of normalized adjacency matricies. \"\"\" # build degree vector d = W . sum ( 1 ) # normalize D_invsqrt = torch . diag_embed ( 1.0 / torch . sqrt ( torch . max ( torch . ones ( d . size ()), d ))) W_norm = D_invsqrt . matmul ( W ) . matmul ( D_invsqrt ) return W_norm normalize_laplacian \u00b6 normalize_laplacian ( L ) Normalize an graph Laplacian. Parameters: Name Type Description Default L torch . Tensor Batch of graph Laplacians. required Returns: Type Description torch . Tensor Batch of normalized graph Laplacians. Source code in gsxform/graph.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def normalize_laplacian ( L : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize an graph Laplacian. Parameters ---------- L: torch.Tensor Batch of graph Laplacians. Returns ------- torch.Tensor Batch of normalized graph Laplacians. \"\"\" # build degree vector # batch diagonal # (https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal) d = torch . diagonal ( L , dim1 =- 2 , dim2 =- 1 ) # normalize D_invsqrt = torch . diag_embed ( 1.0 / torch . sqrt ( torch . max ( torch . ones ( d . size ()), d ))) L_norm = D_invsqrt . matmul ( L ) . matmul ( D_invsqrt ) return L_norm compute_spectra \u00b6 compute_spectra ( W ) Compute the spectra of graph Laplacian from its adjacency matrix. Performs an eigendecomposition (w/o assuming additional structure) using torch.linalg.eigh (previously used torch.symeig ) on a normalized graph laplacian. Converts from the adjacency matrix to the laplacian internally. Parameters: Name Type Description Default W torch . Tensor Batch of graph adjacency matricies. required Returns: Type Description Tuple [ torch . Tensor , torch . Tensor ] Batch of eigenvalues and eigenvectors of the graph Laplacian. Source code in gsxform/graph.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def compute_spectra ( W : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Compute the spectra of graph Laplacian from its adjacency matrix. Performs an eigendecomposition (w/o assuming additional structure) using `torch.linalg.eigh` (previously used `torch.symeig`) on a normalized graph laplacian. Converts from the adjacency matrix to the laplacian internally. Parameters ---------- W: torch.Tensor Batch of graph adjacency matricies. Returns ------- Tuple[torch.Tensor, torch.Tensor] Batch of eigenvalues and eigenvectors of the graph Laplacian. \"\"\" # compute laplacian L = adjacency_to_laplacian ( W ) # normalize laplacian L_norm = normalize_laplacian ( L ) # perform eigen decomp # come out in ascending order, E , V = torch . linalg . eigh ( L_norm , UPLO = \"L\" ) return E , V","title":"Graph utilities"},{"location":"graph/#graph-utility-api-documentation","text":"","title":"Graph utility API documentation"},{"location":"graph/#gsxform.graph.adjacency_to_laplacian","text":"adjacency_to_laplacian ( W ) Convert an adjacency matrix into the graph Laplacian. Parameters: Name Type Description Default W torch . Tensor Batch of normalized graph adjacency matricies. required Returns: Type Description torch . Tensor Batch of graph Laplacians. Source code in gsxform/graph.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def adjacency_to_laplacian ( W : torch . Tensor ) -> torch . Tensor : \"\"\"Convert an adjacency matrix into the graph Laplacian. Parameters ---------- W: torch.Tensor Batch of normalized graph adjacency matricies. Returns ------- torch.Tensor Batch of graph Laplacians. \"\"\" L = torch . diag_embed ( W . sum ( 1 )) - W return L","title":"adjacency_to_laplacian()"},{"location":"graph/#gsxform.graph.normalize_adjacency","text":"normalize_adjacency ( W ) Normalize an adjacency matrix. Parameters: Name Type Description Default W torch . Tensor Batch of adjacency matricies. required Returns: Type Description torch . Tensor Batch of normalized adjacency matricies. Source code in gsxform/graph.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def normalize_adjacency ( W : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize an adjacency matrix. Parameters ---------- W: torch.Tensor Batch of adjacency matricies. Returns ------- torch.Tensor Batch of normalized adjacency matricies. \"\"\" # build degree vector d = W . sum ( 1 ) # normalize D_invsqrt = torch . diag_embed ( 1.0 / torch . sqrt ( torch . max ( torch . ones ( d . size ()), d ))) W_norm = D_invsqrt . matmul ( W ) . matmul ( D_invsqrt ) return W_norm","title":"normalize_adjacency()"},{"location":"graph/#gsxform.graph.normalize_laplacian","text":"normalize_laplacian ( L ) Normalize an graph Laplacian. Parameters: Name Type Description Default L torch . Tensor Batch of graph Laplacians. required Returns: Type Description torch . Tensor Batch of normalized graph Laplacians. Source code in gsxform/graph.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def normalize_laplacian ( L : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize an graph Laplacian. Parameters ---------- L: torch.Tensor Batch of graph Laplacians. Returns ------- torch.Tensor Batch of normalized graph Laplacians. \"\"\" # build degree vector # batch diagonal # (https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal) d = torch . diagonal ( L , dim1 =- 2 , dim2 =- 1 ) # normalize D_invsqrt = torch . diag_embed ( 1.0 / torch . sqrt ( torch . max ( torch . ones ( d . size ()), d ))) L_norm = D_invsqrt . matmul ( L ) . matmul ( D_invsqrt ) return L_norm","title":"normalize_laplacian()"},{"location":"graph/#gsxform.graph.compute_spectra","text":"compute_spectra ( W ) Compute the spectra of graph Laplacian from its adjacency matrix. Performs an eigendecomposition (w/o assuming additional structure) using torch.linalg.eigh (previously used torch.symeig ) on a normalized graph laplacian. Converts from the adjacency matrix to the laplacian internally. Parameters: Name Type Description Default W torch . Tensor Batch of graph adjacency matricies. required Returns: Type Description Tuple [ torch . Tensor , torch . Tensor ] Batch of eigenvalues and eigenvectors of the graph Laplacian. Source code in gsxform/graph.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def compute_spectra ( W : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Compute the spectra of graph Laplacian from its adjacency matrix. Performs an eigendecomposition (w/o assuming additional structure) using `torch.linalg.eigh` (previously used `torch.symeig`) on a normalized graph laplacian. Converts from the adjacency matrix to the laplacian internally. Parameters ---------- W: torch.Tensor Batch of graph adjacency matricies. Returns ------- Tuple[torch.Tensor, torch.Tensor] Batch of eigenvalues and eigenvectors of the graph Laplacian. \"\"\" # compute laplacian L = adjacency_to_laplacian ( W ) # normalize laplacian L_norm = normalize_laplacian ( L ) # perform eigen decomp # come out in ascending order, E , V = torch . linalg . eigh ( L_norm , UPLO = \"L\" ) return E , V","title":"compute_spectra()"},{"location":"install/","text":"Installation \u00b6 gsxform is written without complex dependencies, it can be installed using pip or from source. Currently, gsxform has only been tested with Python3.9, previous versions of Python are not explicitly supported. Using pip \u00b6 gsxform can be installed using pip pip install gsxform From source \u00b6 The code for gsxform can be downloaded and installed as follows: git clone https://github.com/armaank/gsxform.git cd gsxform python setup.py install Development \u00b6 To contribute to gxform , download the source code, setup a conda environment and source the setup script to install all of the pre-commit hooks to ensure appropriate typing and code formatting. git clone https://github.com/armaank/gsxform.git cd gsxform make conda source scripts/setup.sh Please follow the NumPy development workflow naming convention for pull requests. The test suite is run and the documentation site is published automatically on every push to the main branch via Github Actions Testing \u00b6 To run the unit tests locally using pytest , from the root project directory execute make tests Documentation \u00b6 To preview documentation locally, from the root project directory execute: make docs","title":"Installation"},{"location":"install/#installation","text":"gsxform is written without complex dependencies, it can be installed using pip or from source. Currently, gsxform has only been tested with Python3.9, previous versions of Python are not explicitly supported.","title":"Installation"},{"location":"install/#using-pip","text":"gsxform can be installed using pip pip install gsxform","title":"Using pip"},{"location":"install/#from-source","text":"The code for gsxform can be downloaded and installed as follows: git clone https://github.com/armaank/gsxform.git cd gsxform python setup.py install","title":"From source"},{"location":"install/#development","text":"To contribute to gxform , download the source code, setup a conda environment and source the setup script to install all of the pre-commit hooks to ensure appropriate typing and code formatting. git clone https://github.com/armaank/gsxform.git cd gsxform make conda source scripts/setup.sh Please follow the NumPy development workflow naming convention for pull requests. The test suite is run and the documentation site is published automatically on every push to the main branch via Github Actions","title":"Development"},{"location":"install/#testing","text":"To run the unit tests locally using pytest , from the root project directory execute make tests","title":"Testing"},{"location":"install/#documentation","text":"To preview documentation locally, from the root project directory execute: make docs","title":"Documentation"},{"location":"kernel/","text":"Graph kernel functions API documentation \u00b6 TightHannKernel \u00b6 TightHannKernel ( n_scales , max_eig , omega = None ) Bases: object TightHannKernel class. Thie class constructs a spectrum-adaptive tight-hann kernel function used in its corresponding wavelet transform. Based off of the implementation from Tabar et. al 2021 of the algorithm originally described in Shuman et. al 2015 Parameters: Name Type Description Default n_scales int number of scales used in wavelet transform required max_eig torch . Tensor the maximum eigenvalue of the graph laplacian. Used for scaling purposes required omega Union [ Callable [[ torch . Tensor ], torch . Tensor ], None] warping function. Defaults to None None Source code in gsxform/kernel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , n_scales : int , max_eig : torch . Tensor , omega : Union [ Callable [[ torch . Tensor ], torch . Tensor ], None ] = None , ) -> None : \"\"\"Initialize TightHannKernel class Parameters ---------- n_scales: int number of scales used in wavelet transform max_eig: torch.Tensor: the maximum eigenvalue of the graph laplacian. Used for scaling purposes omega: Union[Callable[[torch.Tensor], torch.Tensor], None] warping function. Defaults to None \"\"\" self . n_scales = n_scales self . K = 1 self . R = 3.0 self . max_eig = max_eig if omega is not None : self . omega = omega self . max_eig = self . omega ( self . max_eig . float ()) # dilation factor, might need to reverse this to account for swapped bounds... # self.d = (self.M + 1 - self.R) / (self.R * self.max_eig) self . d = self . R * self . max_eig / ( self . n_scales + 1 - self . R ) # hann kernel functional form self . kernel : Callable [[ torch . Tensor ], torch . Tensor ] = ( lambda eig : sum ( [ 0.5 * torch . cos ( 2 * np . pi * ( eig / self . d - 0.5 ) * k ) for k in range ( self . K + 1 ) ] ) * ( eig >= 0 ) * ( eig <= self . d ) ) get_adapted_kernel \u00b6 get_adapted_kernel ( eig , scale ) compute spectrum adapted kernels. return self.kernel(self.omega(eig) - self.d / self.R * (scale - self.R + 1)) Parameters: Name Type Description Default eig torch . Tensor input tensor of eigenvalues of the graph laplacian required scale int The scale parameter of the specific kernel. Not to be confused with n_scales , which is the total number of scales used by the wavelet transform. required Returns: Name Type Description adapted_kernel torch . Tensor scale-specific adapted kernel Source code in gsxform/kernel.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_adapted_kernel ( self , eig : torch . Tensor , scale : int ) -> torch . Tensor : \"\"\"compute spectrum adapted kernels. return self.kernel(self.omega(eig) - self.d / self.R * (scale - self.R + 1)) Parameters ---------- eig: torch.Tensor input tensor of eigenvalues of the graph laplacian scale: int The scale parameter of the specific kernel. Not to be confused with `n_scales`, which is the total number of scales used by the wavelet transform. Returns -------- adapted_kernel: torch.Tensor scale-specific adapted kernel \"\"\" adapted_kernel = self . kernel ( self . omega ( eig ) - self . d / self . R * ( scale - self . R + 1 ) ) return adapted_kernel","title":"Kernel functions"},{"location":"kernel/#graph-kernel-functions-api-documentation","text":"","title":"Graph kernel functions API documentation"},{"location":"kernel/#gsxform.kernel.TightHannKernel","text":"TightHannKernel ( n_scales , max_eig , omega = None ) Bases: object TightHannKernel class. Thie class constructs a spectrum-adaptive tight-hann kernel function used in its corresponding wavelet transform. Based off of the implementation from Tabar et. al 2021 of the algorithm originally described in Shuman et. al 2015 Parameters: Name Type Description Default n_scales int number of scales used in wavelet transform required max_eig torch . Tensor the maximum eigenvalue of the graph laplacian. Used for scaling purposes required omega Union [ Callable [[ torch . Tensor ], torch . Tensor ], None] warping function. Defaults to None None Source code in gsxform/kernel.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , n_scales : int , max_eig : torch . Tensor , omega : Union [ Callable [[ torch . Tensor ], torch . Tensor ], None ] = None , ) -> None : \"\"\"Initialize TightHannKernel class Parameters ---------- n_scales: int number of scales used in wavelet transform max_eig: torch.Tensor: the maximum eigenvalue of the graph laplacian. Used for scaling purposes omega: Union[Callable[[torch.Tensor], torch.Tensor], None] warping function. Defaults to None \"\"\" self . n_scales = n_scales self . K = 1 self . R = 3.0 self . max_eig = max_eig if omega is not None : self . omega = omega self . max_eig = self . omega ( self . max_eig . float ()) # dilation factor, might need to reverse this to account for swapped bounds... # self.d = (self.M + 1 - self.R) / (self.R * self.max_eig) self . d = self . R * self . max_eig / ( self . n_scales + 1 - self . R ) # hann kernel functional form self . kernel : Callable [[ torch . Tensor ], torch . Tensor ] = ( lambda eig : sum ( [ 0.5 * torch . cos ( 2 * np . pi * ( eig / self . d - 0.5 ) * k ) for k in range ( self . K + 1 ) ] ) * ( eig >= 0 ) * ( eig <= self . d ) )","title":"TightHannKernel"},{"location":"kernel/#gsxform.kernel.TightHannKernel.get_adapted_kernel","text":"get_adapted_kernel ( eig , scale ) compute spectrum adapted kernels. return self.kernel(self.omega(eig) - self.d / self.R * (scale - self.R + 1)) Parameters: Name Type Description Default eig torch . Tensor input tensor of eigenvalues of the graph laplacian required scale int The scale parameter of the specific kernel. Not to be confused with n_scales , which is the total number of scales used by the wavelet transform. required Returns: Name Type Description adapted_kernel torch . Tensor scale-specific adapted kernel Source code in gsxform/kernel.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_adapted_kernel ( self , eig : torch . Tensor , scale : int ) -> torch . Tensor : \"\"\"compute spectrum adapted kernels. return self.kernel(self.omega(eig) - self.d / self.R * (scale - self.R + 1)) Parameters ---------- eig: torch.Tensor input tensor of eigenvalues of the graph laplacian scale: int The scale parameter of the specific kernel. Not to be confused with `n_scales`, which is the total number of scales used by the wavelet transform. Returns -------- adapted_kernel: torch.Tensor scale-specific adapted kernel \"\"\" adapted_kernel = self . kernel ( self . omega ( eig ) - self . d / self . R * ( scale - self . R + 1 ) ) return adapted_kernel","title":"get_adapted_kernel()"},{"location":"scattering/","text":"Scattering transform API documentation \u00b6 ScatteringTransform \u00b6 ScatteringTransform ( W_adj , n_scales , n_layers , nlin = torch . abs , ** kwargs ) Bases: nn . Module ScatteringTransform base class. Inherits from PyTorch nn.Module This class implements the base logic to compute graph scattering transforms with a pooling and an arbitrary wavelet transform operators. This is a base class, and implements only the logic to compute an arbitrary scattering transform. The method get_wavelets must be implemented by the subclass Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs **kwargs Any Additional keyword arguments {} Source code in gsxform/scattering.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , ** kwargs : Any , ) -> None : \"\"\"Initialize scattering transform base class This is a base class, and implements only the logic to compute an arbitrary scattering transform. The method `get_wavelets` must be implemented by the subclass Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable Non-linearity used in the scattering transform. Defaults to torch.abs **kwargs: Any Additional keyword arguments \"\"\" super ( ScatteringTransform , self ) . __init__ () # adjacency matrix self . W_adj = W_adj # number of scales self . n_scales = n_scales # number of layers self . n_layers = n_layers self . n_nodes = self . W_adj . shape [ 1 ] assert self . W_adj . shape [ 1 ] == self . W_adj . shape [ 2 ] self . nlin = nlin # batch size self . b_size = self . W_adj . shape [ 0 ] forward \u00b6 forward ( x ) Forward pass of a generic scattering transform. Parameters: Name Type Description Default x torch . Tensor input batch of graph signals required Returns: Name Type Description phi torch . Tensor scattering representation of the input batch Source code in gsxform/scattering.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of a generic scattering transform. Parameters ---------- x: torch.Tensor input batch of graph signals Returns ------- phi: torch.Tensor scattering representation of the input batch \"\"\" batch_size = x . shape [ 0 ] n_features = x . shape [ 1 ] lowpass = self . get_lowpass () psi = self . get_wavelets () # compute first scattering layer, low pass filter via matmul phi = torch . matmul ( x , lowpass ) # reshape inputs for loop S_x = rearrange ( x , \"b f n -> b 1 f n\" ) lowpass = rearrange ( lowpass , \"b n 1 -> b 1 n 1\" ) lowpass = repeat ( lowpass , \"b 1 n 1 -> b (1 ns) n 1\" , ns = self . n_scales ) for ll in range ( 1 , self . n_layers ): S_x_ll = torch . empty ([ batch_size , 0 , n_features , self . n_nodes ]) for jj in range ( self . n_scales ** ( ll - 1 )): # intermediate repr x_jj = rearrange ( S_x [:, jj , :, :], \"b f n -> b 1 f n\" ) # wavelet filtering operation, matrix multiply psi_x_jj = torch . matmul ( x_jj , psi ) # application of non-linearity, yields scattering output S_x_jj = self . nlin ( psi_x_jj ) # concat scattering scale for the layer S_x_ll = torch . cat (( S_x_ll , S_x_jj ), axis = 1 ) # compute scattering representation, matrix multiply phi_jj = torch . matmul ( S_x_jj , lowpass ) phi_jj = rearrange ( phi_jj , \"b l f 1 -> b f l\" ) phi = torch . cat (( phi , phi_jj ), axis = 2 ) S_x = S_x_ll . clone () # continue iteration through the layer return phi get_lowpass \u00b6 get_lowpass () Compute lowpass filtering/pooling operator. This should roughly resemble an average, it alters the output scaling factor. For instance averaging with the norm of the degree vector scales towards zero, this implementation offers a more natural scaling. Returns: Name Type Description lowpass torch . Tensor average pooling operator Source code in gsxform/scattering.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def get_lowpass ( self ) -> torch . Tensor : \"\"\"Compute lowpass filtering/pooling operator. This should roughly resemble an average, it alters the output scaling factor. For instance averaging with the norm of the degree vector scales towards zero, this implementation offers a more natural scaling. Returns ------- lowpass: torch.Tensor average pooling operator \"\"\" lowpass = ( 1 / self . n_nodes ) * torch . ones ( self . b_size , self . n_nodes ) lowpass = rearrange ( lowpass , \"b ni -> b ni 1\" ) return lowpass get_wavelets \u00b6 get_wavelets () Compute wavelet operator. Subclasses are required to implement this method Source code in gsxform/scattering.py 71 72 73 74 75 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Compute wavelet operator. Subclasses are required to implement this method\"\"\" raise NotImplementedError Diffusion \u00b6 Diffusion ( W_adj , n_scales , n_layers , nlin = torch . abs ) Bases: ScatteringTransform Diffusion scattering transform. Subclass of ScatteringTransform , implements get_wavelets method. Diffusion scattering transform algorithm based on description in Gama et. al 2018. Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs Source code in gsxform/scattering.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , ) -> None : \"\"\"Initialize diffusion scattering transform Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable[torch.Tensor] Non-linearity used in the scattering transform. Defaults to torch.abs \"\"\" super () . __init__ ( W_adj , n_scales , n_layers , nlin ) get_wavelets \u00b6 get_wavelets () Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns: Name Type Description psi torch . Tensor diffusion wavelet operator Source code in gsxform/scattering.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns ------- psi: torch.Tensor diffusion wavelet operator \"\"\" W_norm = normalize_adjacency ( self . W_adj ) # compute diffusion matrix T = 1 / 2 * ( torch . eye ( self . n_nodes ) + W_norm ) # compute wavelet operator psi = diffusion_wavelets ( T , self . n_scales ) return psi TightHann \u00b6 TightHann ( W_adj , n_scales , n_layers , nlin = torch . abs , use_warp = True ) Bases: ScatteringTransform TightHann scattering transform. Subclass of ScatteringTransform , implements get_wavelets methods. Also additionally implements functions used to compute spectrum-adaptive wavelets. Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs use_warp bool Use warping function. Defaults to True True Source code in gsxform/scattering.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , use_warp : bool = True , ) -> None : \"\"\"Initialize diffusion scattering transform Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable[torch.Tensor] Non-linearity used in the scattering transform. Defaults to torch.abs use_warp: bool Use warping function. Defaults to True \"\"\" super () . __init__ ( W_adj , n_scales , n_layers , nlin ) self . use_warp = use_warp self . warp = self . warp_func () get_kernel \u00b6 get_kernel () compute TightHann kernel adaptively Source code in gsxform/scattering.py 264 265 266 267 268 269 def get_kernel ( self ) -> TightHannKernel : \"\"\"compute TightHann kernel adaptively\"\"\" omega = lambda eig : torch . tensor ( self . warp ( eig . numpy ())) return TightHannKernel ( self . n_scales , self . max_eig , omega ) get_wavelets \u00b6 get_wavelets () Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns: Name Type Description psi torch . Tensor diffusion wavelet operator Source code in gsxform/scattering.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns ------- psi: torch.Tensor diffusion wavelet operator \"\"\" # compute wavelet operator psi = tighthann_wavelets ( self . W_adj , self . n_scales , self . get_kernel ()) return psi warp_func \u00b6 warp_func () Implements spectrum-adaptive warping function Source code in gsxform/scattering.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def warp_func ( self ) -> torch . Tensor : \"\"\"Implements spectrum-adaptive warping function\"\"\" E , V = compute_spectra ( self . W_adj ) self . spectra , _ = torch . sort ( E . reshape ( - 1 )) # change this self . max_eig = self . spectra . max () cdf = torch . arange ( 0 , len ( self . spectra )) / ( len ( self . spectra ) - 1.0 ) step = int ( len ( self . spectra ) / 5 - 1 ) if self . use_warp : return interp1d ( self . spectra [ 0 :: step ], cdf [ 0 :: step ], fill_value = \"extrapolate\" ) else : return interp1d ( self . spectra , cdf , fill_value = \"extrapolate\" )","title":"Graph scattering transforms"},{"location":"scattering/#scattering-transform-api-documentation","text":"","title":"Scattering transform API documentation"},{"location":"scattering/#gsxform.scattering.ScatteringTransform","text":"ScatteringTransform ( W_adj , n_scales , n_layers , nlin = torch . abs , ** kwargs ) Bases: nn . Module ScatteringTransform base class. Inherits from PyTorch nn.Module This class implements the base logic to compute graph scattering transforms with a pooling and an arbitrary wavelet transform operators. This is a base class, and implements only the logic to compute an arbitrary scattering transform. The method get_wavelets must be implemented by the subclass Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs **kwargs Any Additional keyword arguments {} Source code in gsxform/scattering.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , ** kwargs : Any , ) -> None : \"\"\"Initialize scattering transform base class This is a base class, and implements only the logic to compute an arbitrary scattering transform. The method `get_wavelets` must be implemented by the subclass Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable Non-linearity used in the scattering transform. Defaults to torch.abs **kwargs: Any Additional keyword arguments \"\"\" super ( ScatteringTransform , self ) . __init__ () # adjacency matrix self . W_adj = W_adj # number of scales self . n_scales = n_scales # number of layers self . n_layers = n_layers self . n_nodes = self . W_adj . shape [ 1 ] assert self . W_adj . shape [ 1 ] == self . W_adj . shape [ 2 ] self . nlin = nlin # batch size self . b_size = self . W_adj . shape [ 0 ]","title":"ScatteringTransform"},{"location":"scattering/#gsxform.scattering.ScatteringTransform.forward","text":"forward ( x ) Forward pass of a generic scattering transform. Parameters: Name Type Description Default x torch . Tensor input batch of graph signals required Returns: Name Type Description phi torch . Tensor scattering representation of the input batch Source code in gsxform/scattering.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward pass of a generic scattering transform. Parameters ---------- x: torch.Tensor input batch of graph signals Returns ------- phi: torch.Tensor scattering representation of the input batch \"\"\" batch_size = x . shape [ 0 ] n_features = x . shape [ 1 ] lowpass = self . get_lowpass () psi = self . get_wavelets () # compute first scattering layer, low pass filter via matmul phi = torch . matmul ( x , lowpass ) # reshape inputs for loop S_x = rearrange ( x , \"b f n -> b 1 f n\" ) lowpass = rearrange ( lowpass , \"b n 1 -> b 1 n 1\" ) lowpass = repeat ( lowpass , \"b 1 n 1 -> b (1 ns) n 1\" , ns = self . n_scales ) for ll in range ( 1 , self . n_layers ): S_x_ll = torch . empty ([ batch_size , 0 , n_features , self . n_nodes ]) for jj in range ( self . n_scales ** ( ll - 1 )): # intermediate repr x_jj = rearrange ( S_x [:, jj , :, :], \"b f n -> b 1 f n\" ) # wavelet filtering operation, matrix multiply psi_x_jj = torch . matmul ( x_jj , psi ) # application of non-linearity, yields scattering output S_x_jj = self . nlin ( psi_x_jj ) # concat scattering scale for the layer S_x_ll = torch . cat (( S_x_ll , S_x_jj ), axis = 1 ) # compute scattering representation, matrix multiply phi_jj = torch . matmul ( S_x_jj , lowpass ) phi_jj = rearrange ( phi_jj , \"b l f 1 -> b f l\" ) phi = torch . cat (( phi , phi_jj ), axis = 2 ) S_x = S_x_ll . clone () # continue iteration through the layer return phi","title":"forward()"},{"location":"scattering/#gsxform.scattering.ScatteringTransform.get_lowpass","text":"get_lowpass () Compute lowpass filtering/pooling operator. This should roughly resemble an average, it alters the output scaling factor. For instance averaging with the norm of the degree vector scales towards zero, this implementation offers a more natural scaling. Returns: Name Type Description lowpass torch . Tensor average pooling operator Source code in gsxform/scattering.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def get_lowpass ( self ) -> torch . Tensor : \"\"\"Compute lowpass filtering/pooling operator. This should roughly resemble an average, it alters the output scaling factor. For instance averaging with the norm of the degree vector scales towards zero, this implementation offers a more natural scaling. Returns ------- lowpass: torch.Tensor average pooling operator \"\"\" lowpass = ( 1 / self . n_nodes ) * torch . ones ( self . b_size , self . n_nodes ) lowpass = rearrange ( lowpass , \"b ni -> b ni 1\" ) return lowpass","title":"get_lowpass()"},{"location":"scattering/#gsxform.scattering.ScatteringTransform.get_wavelets","text":"get_wavelets () Compute wavelet operator. Subclasses are required to implement this method Source code in gsxform/scattering.py 71 72 73 74 75 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Compute wavelet operator. Subclasses are required to implement this method\"\"\" raise NotImplementedError","title":"get_wavelets()"},{"location":"scattering/#gsxform.scattering.Diffusion","text":"Diffusion ( W_adj , n_scales , n_layers , nlin = torch . abs ) Bases: ScatteringTransform Diffusion scattering transform. Subclass of ScatteringTransform , implements get_wavelets method. Diffusion scattering transform algorithm based on description in Gama et. al 2018. Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs Source code in gsxform/scattering.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , ) -> None : \"\"\"Initialize diffusion scattering transform Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable[torch.Tensor] Non-linearity used in the scattering transform. Defaults to torch.abs \"\"\" super () . __init__ ( W_adj , n_scales , n_layers , nlin )","title":"Diffusion"},{"location":"scattering/#gsxform.scattering.Diffusion.get_wavelets","text":"get_wavelets () Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns: Name Type Description psi torch . Tensor diffusion wavelet operator Source code in gsxform/scattering.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns ------- psi: torch.Tensor diffusion wavelet operator \"\"\" W_norm = normalize_adjacency ( self . W_adj ) # compute diffusion matrix T = 1 / 2 * ( torch . eye ( self . n_nodes ) + W_norm ) # compute wavelet operator psi = diffusion_wavelets ( T , self . n_scales ) return psi","title":"get_wavelets()"},{"location":"scattering/#gsxform.scattering.TightHann","text":"TightHann ( W_adj , n_scales , n_layers , nlin = torch . abs , use_warp = True ) Bases: ScatteringTransform TightHann scattering transform. Subclass of ScatteringTransform , implements get_wavelets methods. Also additionally implements functions used to compute spectrum-adaptive wavelets. Parameters: Name Type Description Default W_adj torch . Tensor Weighted adjacency matrix required n_scales int Number of scales to use in wavelet transform required n_layers int Number of layers in the scattering transform required nlin Callable [[ torch . Tensor ], torch . Tensor ] Non-linearity used in the scattering transform. Defaults to torch.abs torch.abs use_warp bool Use warping function. Defaults to True True Source code in gsxform/scattering.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __init__ ( self , W_adj : torch . Tensor , n_scales : int , n_layers : int , nlin : Callable [[ torch . Tensor ], torch . Tensor ] = torch . abs , use_warp : bool = True , ) -> None : \"\"\"Initialize diffusion scattering transform Parameters ---------- W_adj: torch.Tensor Weighted adjacency matrix n_scales: int Number of scales to use in wavelet transform n_layers: int Number of layers in the scattering transform nlin: Callable[torch.Tensor] Non-linearity used in the scattering transform. Defaults to torch.abs use_warp: bool Use warping function. Defaults to True \"\"\" super () . __init__ ( W_adj , n_scales , n_layers , nlin ) self . use_warp = use_warp self . warp = self . warp_func ()","title":"TightHann"},{"location":"scattering/#gsxform.scattering.TightHann.get_kernel","text":"get_kernel () compute TightHann kernel adaptively Source code in gsxform/scattering.py 264 265 266 267 268 269 def get_kernel ( self ) -> TightHannKernel : \"\"\"compute TightHann kernel adaptively\"\"\" omega = lambda eig : torch . tensor ( self . warp ( eig . numpy ())) return TightHannKernel ( self . n_scales , self . max_eig , omega )","title":"get_kernel()"},{"location":"scattering/#gsxform.scattering.TightHann.get_wavelets","text":"get_wavelets () Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns: Name Type Description psi torch . Tensor diffusion wavelet operator Source code in gsxform/scattering.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def get_wavelets ( self ) -> torch . Tensor : \"\"\"Subclass method used to get wavelet filter bank This method returns diffusion wavelets Returns ------- psi: torch.Tensor diffusion wavelet operator \"\"\" # compute wavelet operator psi = tighthann_wavelets ( self . W_adj , self . n_scales , self . get_kernel ()) return psi","title":"get_wavelets()"},{"location":"scattering/#gsxform.scattering.TightHann.warp_func","text":"warp_func () Implements spectrum-adaptive warping function Source code in gsxform/scattering.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def warp_func ( self ) -> torch . Tensor : \"\"\"Implements spectrum-adaptive warping function\"\"\" E , V = compute_spectra ( self . W_adj ) self . spectra , _ = torch . sort ( E . reshape ( - 1 )) # change this self . max_eig = self . spectra . max () cdf = torch . arange ( 0 , len ( self . spectra )) / ( len ( self . spectra ) - 1.0 ) step = int ( len ( self . spectra ) / 5 - 1 ) if self . use_warp : return interp1d ( self . spectra [ 0 :: step ], cdf [ 0 :: step ], fill_value = \"extrapolate\" ) else : return interp1d ( self . spectra , cdf , fill_value = \"extrapolate\" )","title":"warp_func()"},{"location":"wavelets/","text":"Graph wavelets API documentation \u00b6 diffusion_wavelets \u00b6 diffusion_wavelets ( T , n_scales ) Compute diffusion wavelet filter bank Computes diffusion wavelets from from input diffusion matrix. Implementation based off the algorithm originally described in Coifman et. al 2006. Parameters: Name Type Description Default T torch . Tensor Input diffusion matrix computed from adjacency matrix required n_scales int Number of scales to use in wavelet transform required Returns: Name Type Description phi torch . Tensor wavelet filter bank Source code in gsxform/wavelets.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def diffusion_wavelets ( T : torch . Tensor , n_scales : int ) -> torch . Tensor : \"\"\"Compute diffusion wavelet filter bank Computes diffusion wavelets from from input diffusion matrix. Implementation based off the algorithm originally described in Coifman et. al 2006. Parameters ---------- T: torch.Tensor Input diffusion matrix computed from adjacency matrix n_scales: int Number of scales to use in wavelet transform Returns ------- phi: torch.Tensor wavelet filter bank \"\"\" # make n_node x n_node identity matrix I_N = torch . eye ( T . shape [ 1 ]) # compute zero-eth order (J=0) wavelet filter # one half the normalized laplacian operator 1/2(I-D^-1/2WD^-1/2) psi = I_N - T for jj in range ( 1 , n_scales ): # compute jth diffusion operator (wavelet kernel) T_j = torch . matrix_power ( T , 2 ** ( jj - 1 )) # compute jth wavelet filter via matmul # psi_j = torch.einsum(\"b n m, b n m -> b n m\", T_j, (I_N - T_j)) psi_j = torch . matmul ( T_j , ( I_N - T_j )) # append wavelets psi = torch . cat (( psi , psi_j ), axis = 0 ) psi = rearrange ( psi , \"(b ns) ni nj -> b ns ni nj\" , ns = n_scales ) return psi tighthann_wavelets \u00b6 tighthann_wavelets ( W_adj , n_scales , kernel ) Computes spectrum adapted tight Hann wavelets. Based of algorithm described in Shuman et. al 2015. Parameters: Name Type Description Default W_adj torch . Tensor Input batch of adjacency matricies required n_scales int Number of scales to use in wavelet transform required kernel TightHannKernel Adaptive kernel used in wavelet transform. required Returns: Name Type Description psi torch . Tensor wavelet filter bank Source code in gsxform/wavelets.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tighthann_wavelets ( W_adj : torch . Tensor , n_scales : int , kernel : TightHannKernel ) -> torch . Tensor : \"\"\"Computes spectrum adapted tight Hann wavelets. Based of algorithm described in Shuman et. al 2015. Parameters ---------- W_adj: torch.Tensor Input batch of adjacency matricies n_scales: int Number of scales to use in wavelet transform kernel: TightHannKernel Adaptive kernel used in wavelet transform. Returns ------- psi: torch.Tensor wavelet filter bank \"\"\" E , V = compute_spectra ( W_adj ) V_herm = rearrange ( V , \"b ni nj -> b nj ni\" ) # hermetian transpose # compute wavelet coeffs psi = torch . empty ( V . shape [ 0 ], 0 , V . shape [ 1 ], V . shape [ 2 ]) for jj in range ( 0 , n_scales ): # compute adapted kernel adapted_kernel = kernel . get_adapted_kernel ( E , jj + 1 ) phi = torch . diag_embed ( adapted_kernel ) # compute jth wavelet filter via matmul psi_j = V . matmul ( phi ) . matmul ( V_herm ) # append wavelets psi_j = rearrange ( psi_j , \"b n m -> b 1 n m\" ) psi = torch . cat (( psi , psi_j ), axis = 1 ) return psi","title":"Graph wavelet transforms"},{"location":"wavelets/#graph-wavelets-api-documentation","text":"","title":"Graph wavelets API documentation"},{"location":"wavelets/#gsxform.wavelets.diffusion_wavelets","text":"diffusion_wavelets ( T , n_scales ) Compute diffusion wavelet filter bank Computes diffusion wavelets from from input diffusion matrix. Implementation based off the algorithm originally described in Coifman et. al 2006. Parameters: Name Type Description Default T torch . Tensor Input diffusion matrix computed from adjacency matrix required n_scales int Number of scales to use in wavelet transform required Returns: Name Type Description phi torch . Tensor wavelet filter bank Source code in gsxform/wavelets.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def diffusion_wavelets ( T : torch . Tensor , n_scales : int ) -> torch . Tensor : \"\"\"Compute diffusion wavelet filter bank Computes diffusion wavelets from from input diffusion matrix. Implementation based off the algorithm originally described in Coifman et. al 2006. Parameters ---------- T: torch.Tensor Input diffusion matrix computed from adjacency matrix n_scales: int Number of scales to use in wavelet transform Returns ------- phi: torch.Tensor wavelet filter bank \"\"\" # make n_node x n_node identity matrix I_N = torch . eye ( T . shape [ 1 ]) # compute zero-eth order (J=0) wavelet filter # one half the normalized laplacian operator 1/2(I-D^-1/2WD^-1/2) psi = I_N - T for jj in range ( 1 , n_scales ): # compute jth diffusion operator (wavelet kernel) T_j = torch . matrix_power ( T , 2 ** ( jj - 1 )) # compute jth wavelet filter via matmul # psi_j = torch.einsum(\"b n m, b n m -> b n m\", T_j, (I_N - T_j)) psi_j = torch . matmul ( T_j , ( I_N - T_j )) # append wavelets psi = torch . cat (( psi , psi_j ), axis = 0 ) psi = rearrange ( psi , \"(b ns) ni nj -> b ns ni nj\" , ns = n_scales ) return psi","title":"diffusion_wavelets()"},{"location":"wavelets/#gsxform.wavelets.tighthann_wavelets","text":"tighthann_wavelets ( W_adj , n_scales , kernel ) Computes spectrum adapted tight Hann wavelets. Based of algorithm described in Shuman et. al 2015. Parameters: Name Type Description Default W_adj torch . Tensor Input batch of adjacency matricies required n_scales int Number of scales to use in wavelet transform required kernel TightHannKernel Adaptive kernel used in wavelet transform. required Returns: Name Type Description psi torch . Tensor wavelet filter bank Source code in gsxform/wavelets.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tighthann_wavelets ( W_adj : torch . Tensor , n_scales : int , kernel : TightHannKernel ) -> torch . Tensor : \"\"\"Computes spectrum adapted tight Hann wavelets. Based of algorithm described in Shuman et. al 2015. Parameters ---------- W_adj: torch.Tensor Input batch of adjacency matricies n_scales: int Number of scales to use in wavelet transform kernel: TightHannKernel Adaptive kernel used in wavelet transform. Returns ------- psi: torch.Tensor wavelet filter bank \"\"\" E , V = compute_spectra ( W_adj ) V_herm = rearrange ( V , \"b ni nj -> b nj ni\" ) # hermetian transpose # compute wavelet coeffs psi = torch . empty ( V . shape [ 0 ], 0 , V . shape [ 1 ], V . shape [ 2 ]) for jj in range ( 0 , n_scales ): # compute adapted kernel adapted_kernel = kernel . get_adapted_kernel ( E , jj + 1 ) phi = torch . diag_embed ( adapted_kernel ) # compute jth wavelet filter via matmul psi_j = V . matmul ( phi ) . matmul ( V_herm ) # append wavelets psi_j = rearrange ( psi_j , \"b n m -> b 1 n m\" ) psi = torch . cat (( psi , psi_j ), axis = 1 ) return psi","title":"tighthann_wavelets()"},{"location":"examples/example/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import torch","title":"Basic Plot"}]}